twitter combined text file  : wget https://drive.google.com/u/0/uc?id=1euPO8FMBEMAUyzYilRGkxFAzKPU2t_JB
                                mv uc?id=1euPO8FMBEMAUyzYilRGkxFAzKPU2t_JB twitter_combined.txt
                                hdfs dfs -put twitter_combined.txt /user/hadoop/

facebook text file :    wget https://drive.google.com/u/0/uc?id=1G4R2RqsPAMHcv2nm-P80GfexLILucZyj
                        mv uc?id=1G4R2RqsPAMHcv2nm-P80GfexLILucZyj facebook_combined.txt
                        hdfs dfs -put facebook_combined.txt /user/hadoop/


googe web text file : wget https://drive.google.com/u/0/uc?id=1C_AKKrr0wkNvJHoqVl9_a5eva-YKZhL1
                          mv uc?id=1C_AKKrr0wkNvJHoqVl9_a5eva-YKZhL1 web-google.txt
                        hdfs dfs -put web-google.txt /user/hadoop/

test file from example : touch test.txt
                            nano test.txt

                           <copy file content from python>
                            hdfs dfs -put test.txt /user/hadoop/

test source : touch test.py
                nano test.py
            <from pyspark import SparkContext, SparkConf
conf = SparkConf().setAppName("pyspark")
sc = SparkContext(conf=conf)

dataRDD = sc.textFile("test.txt")
for line in dataRDD.collect():
    print(line)
dataRDD.saveAsTextFile("sortieTestFile")>

test CcfIterate.py : touch HadoopPySparkCcfIterate.py
                    <copy file content>

Ccf with secondary sorting lien :  wget https://drive.google.com/u/0/uc?id=1ueaPymyEsw0HIHt3Js_s4OYdA0tiOmj1
                                    mv uc?id=1ueaPymyEsw0HIHt3Js_s4OYdA0tiOmj1 CcfIterateWIthSorting.py


pour modifier le fichier : nano test.py
pour lancer le spark :

spark-submit --master yarn --executor-memory 10G --driver-memory 10G  test.py
spark-submit --executor-memory 10G  test.py
spark-submit --executor-memory 4G --driver-memory 10G
spark-submit --master yarn    --deploy-mode cluster     --driver-memory 4g     --executor-memory 2g     --executor-cores 1 HadoopPySparkCcfIterate.py
spark-submit --master yarn    --deploy-mode cluster    --num-executors 5  --driver-memory 2g     --executor-memory 2g     --executor-cores 1  HadoopPySparkCcfIterate.py
spark-submit --master yarn    --deploy-mode cluster    --num-executors 10  --driver-memory 5500M     --executor-memory 1900M     --executor-cores 1  HadoopPySparkCcfIterate.py



consulter les fichiers de sortie : hdfs dfs -cat /user/hadoop/sortieCcfIterate/part*
afficher toutes l'arborecence des fichiers de sortie : hdfs dfs -ls /user/hadoop/
                                    un fichier sp√©cifique : hdfs dfs -ls /user/hadoop/sortieCcfIterate/