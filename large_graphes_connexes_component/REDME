twitter combined text file  : wget https://drive.google.com/u/0/uc?id=1euPO8FMBEMAUyzYilRGkxFAzKPU2t_JB
                                mv uc?id=1euPO8FMBEMAUyzYilRGkxFAzKPU2t_JB twitter_combined.txt
                                hdfs dfs -put twitter_combined.txt /user/hadoop/

facebook text file :    wget https://drive.google.com/u/0/uc?id=1G4R2RqsPAMHcv2nm-P80GfexLILucZyj
                        mv uc?id=1G4R2RqsPAMHcv2nm-P80GfexLILucZyj facebook_combined.txt
                        hdfs dfs -put facebook_combined.txt /user/hadoop/

test file from example : touch test.txt
                            nano test.txt
                           <copy file content from python>
                            hdfs dfs -put test.txt /user/hadoop/

test source : touch test.py
                nano test.py
            <from pyspark import SparkContext, SparkConf
conf = SparkConf().setAppName("pyspark")
sc = SparkContext(conf=conf)

dataRDD = sc.textFile("test.txt")
for line in dataRDD.collect():
    print(line)
dataRDD.saveAsTextFile("sortieTestFile")>

test CcfIterate.py : touch HadoopPySparkCcfIterate.py
                    <copy file content>

Ccf with secondary sorting lien :  wget https://drive.google.com/u/0/uc?id=1ueaPymyEsw0HIHt3Js_s4OYdA0tiOmj1
                                    mv uc?id=1ueaPymyEsw0HIHt3Js_s4OYdA0tiOmj1 CcfIterateWIthSorting.py


pour modifier le fichier : nano test.py
pour lancer le spark : spark-submit --master yarn test.py


consulter les fichiers de sortie : hdfs dfs -cat /user/hadoop/sortieCcfIterate/part*
afficher toutes l'arborecence des fichiers de sortie : hdfs dfs -ls /user/hadoop/
                                    un fichier sp√©cifique : hdfs dfs -ls /user/hadoop/sortieCcfIterate/